{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwxgdZjSfg_m"
   },
   "source": [
    "## Prepare module and drive\n",
    "This training script will train model that have 3 types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23298,
     "status": "ok",
     "timestamp": 1713702621161,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "7cMOKRd7TkGk",
    "outputId": "3b5aa6ce-7536-41e6-b8ed-2a1ac37de68d"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import aubio\n",
    "import soundfile as sf\n",
    "from soundfile import write\n",
    "from numpy import vstack, zeros, diff\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold , KFold ,RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.layers import SimpleRNN, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKsMDywYeEza"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "506xGRnRdsF_"
   },
   "source": [
    "- dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time mask method\n",
    "TMASK_MIN = 0\n",
    "TMASK_MAX = 0.07\n",
    "TLEN_MIN = 0.005\n",
    "TLEN_MAX = 0.02\n",
    "\n",
    "#frequency mask method\n",
    "FMASK_MIN = 150\n",
    "FMASK_MAX = 400\n",
    "FLEN_MIN = 20\n",
    "FLEN_MAX = 50\n",
    "\n",
    "\n",
    "AUGMENTATION = 1 #for t_mask f_mask 決定mask數量\n",
    "SR = 22050 #採樣率\n",
    "N_FFT = 128 #Number of FFT Points A smaller N_FFT provides finer time resolution but coarser frequency resolution.\n",
    "HOP_LENGTH = N_FFT / 4\n",
    "# freq = librosa.fft_frequencies(sr=SR, n_fft=2048)\n",
    "# print(freq)\n",
    "# print(len(freq))\n",
    "# print(freq[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataloader(path, file, shift=False, noise=False, t_mask=False, f_mask=False, STFT_noise = False):\n",
    "#     original_audio, sr = librosa.load(path + file)\n",
    "#     audio_list = [original_audio]\n",
    "    \n",
    "#     #time shift\n",
    "#     if shift:\n",
    "#         audio_list.append(np.roll(original_audio, random.randint(1,1000)))\n",
    "#         audio_list.append(np.roll(original_audio, random.randint(1001,2000)))\n",
    "#         # audio_list.append(np.roll(original_audio, random.randint(-500,-1)))\n",
    "    \n",
    "#     #adding noise\n",
    "#     if noise:\n",
    "#         for i in range(len(audio_list)):\n",
    "#             signal = audio_list[i]\n",
    "#             SNR = random.uniform(15, 25)\n",
    "#             # print(SNR)\n",
    "#             Ps = np.sum(np.power(signal, 2)) / len(signal) #信號功率計算\n",
    "#             Pn = Ps / (np.power(10, SNR / 10)) #噪音功率計算\n",
    "#             noise_factor = np.sqrt(Pn)\n",
    "#             white_noise = np.random.randn(len(signal)) * noise_factor\n",
    "#             audio = signal + white_noise\n",
    "#             audio_list.append(audio)\n",
    "            \n",
    "    \n",
    "#     data_list = []\n",
    "#     for audio in audio_list:\n",
    "        \n",
    "#         #normalization\n",
    "#         normalize_data = audio / np.max(np.abs(audio), axis=0)\n",
    "#         stfts = []\n",
    "        \n",
    "#         #stft\n",
    "#         origin_stft = librosa.stft(y=normalize_data, n_fft=N_FFT)\n",
    "#         stfts.append(origin_stft)\n",
    "        \n",
    "#         #time mask\n",
    "#         if t_mask:\n",
    "#             start_point = np.random.uniform(TMASK_MIN, TMASK_MAX, AUGMENTATION)\n",
    "#             mask_len = np.random.uniform(TLEN_MIN, TLEN_MAX, AUGMENTATION)\n",
    "#             # print(start_point, mask_len)\n",
    "#             for i in range(AUGMENTATION):  \n",
    "#                 t_mask_stft = origin_stft.copy()\n",
    "#                 t_mask_stft[:, int(start_point[i] * SR / HOP_LENGTH):int((start_point[i] + mask_len[i]) * SR / HOP_LENGTH)] = 0\n",
    "#                 stfts.append(t_mask_stft)\n",
    "        \n",
    "#         #frequency mask\n",
    "#         if f_mask:\n",
    "#             start_point = np.random.uniform(FMASK_MIN, FMASK_MAX, AUGMENTATION)\n",
    "#             mask_len = np.random.uniform(FLEN_MIN, FLEN_MAX, AUGMENTATION)\n",
    "#             print(start_point, mask_len)\n",
    "#             for i in range(AUGMENTATION):\n",
    "#                 f_mask_stft = origin_stft.copy()\n",
    "#                 # findex = np.argwhere((freq > start_point[i]) & (freq < start_point[i] + mask_len[i]))\n",
    "#                 findex = np.argwhere((freq > start_point[0]) & (freq < start_point[0] + mask_len[0]))\n",
    "#                 print(findex)\n",
    "#                 f_mask_stft[findex[0][0]:findex[len(findex)-1][0], :] = 0\n",
    "#                 print(f_mask_stft.shape)\n",
    "#                 stfts.append(f_mask_stft)\n",
    "                \n",
    "#         for stft in stfts: \n",
    "#             stft = librosa.amplitude_to_db(np.abs(stft),ref=np.max) #轉分貝\n",
    "#             # fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "#             # img1 = librosa.display.specshow(stft, hop_length=64, x_axis='time', y_axis='log', ax=ax)\n",
    "#             # ax.set(title='STFT (log scale)')\n",
    "#             stft = stft[:, :, np.newaxis] #二維到三維拓展\n",
    "#             stft /= np.max(np.abs(stft)) #歸一化\n",
    "#             data_list.append(np.concatenate((stft, stft, stft), axis=2)) #模擬三通道圖像數據：才能適用模型訓練\n",
    "#     return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-pGSqgtsT1kd"
   },
   "outputs": [],
   "source": [
    "N_FFT = 128\n",
    "HOP_LENGTH = 32\n",
    "# image generation method for testing\n",
    "# \"melspectrogram\", \"raw\", \"mfcc_aubio\", \"mfcc_librosa\", \"stft\"\n",
    "image_gen_method = \"mfcc_librosa\"\n",
    "audio_norm = False\n",
    "\n",
    "def mySTFT(y, n_fft: int = 1024, hop_length = 128):\n",
    "    z = np.copy(y)\n",
    "    z = np.pad(z, (n_fft // 2, n_fft // 2), 'constant', constant_values=(0, 0))\n",
    "    result_w = n_fft // 2 + 1\n",
    "    result_h = len(y) // hop_length + 1\n",
    "    result = np.zeros(shape=(result_h, result_w))\n",
    "    hann = np.hanning(n_fft)\n",
    "    for i in range(result_h):\n",
    "        tmp = np.fft.rfft(z[i * hop_length: i * hop_length + n_fft] * hann)\n",
    "        # print(len(tmp))\n",
    "        for j in range(result_w):\n",
    "            result[i][j] = abs(tmp[j])\n",
    "    return result\n",
    "            \n",
    "def randomSTFT(y):\n",
    "    result = np.zeros(shape=(len(y), len(y[0])))\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(y[0])):\n",
    "            result[i][j] = random.random() * 0.2 + y[i][j]\n",
    "    return result\n",
    "    \n",
    "# mask 先不用\n",
    "def dataloader(path, file, shift=False, noise=False, t_mask=False, f_mask=False, STFT_noise=False):\n",
    "    # 讀取原始音訊數據\n",
    "    original_audio, sr = librosa.load(path + file)\n",
    "    audio_list = [original_audio]\n",
    "\n",
    "    # 隨機平移數據擴增 p36\n",
    "    if shift:\n",
    "        audio_list.append(np.roll(original_audio, random.randint(1,1000)))\n",
    "        audio_list.append(np.roll(original_audio, random.randint(1001,2000)))\n",
    "        # audio_list.append(np.roll(original_audio, random.randint(-500,-1)))\n",
    "\n",
    "    # 噪音數據擴增 p37\n",
    "    if noise:\n",
    "        for i in range(len(audio_list)):\n",
    "            signal = audio_list[i]\n",
    "            SNR = random.uniform(15, 25)\n",
    "            # print(SNR)\n",
    "            Ps = np.sum(np.power(signal, 2)) / len(signal)\n",
    "            Pn = Ps / (np.power(10, SNR / 10))\n",
    "            noise_factor = np.sqrt(Pn)\n",
    "            white_noise = np.random.randn(len(signal)) * noise_factor\n",
    "            audio = signal + white_noise\n",
    "            audio_list.append(audio)\n",
    "\n",
    "    features = []\n",
    "    # 遍歷每個音頻文件\n",
    "    for audio in audio_list:\n",
    "        # 提取MFCC特徵\n",
    "        if audio_norm == True:\n",
    "            audio = audio / np.max(np.abs(audio), axis=0)\n",
    "        if image_gen_method == \"melspectrogram\":\n",
    "            spectro = librosa.power_to_db(librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=1024))\n",
    "            spectro /= np.max(np.abs(spectro))\n",
    "            features.append(spectro)\n",
    "        elif image_gen_method == \"mfcc_librosa\":\n",
    "            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=60, hop_length=64)\n",
    "            #mfcc = librosa.feature.mfcc(y=audio, sr=sr, norm=None)\n",
    "            mfcc = mfcc / np.max(np.abs(mfcc), axis=0)\n",
    "            features.append(mfcc)\n",
    "        elif image_gen_method == \"mfcc_aubio\":\n",
    "            win_s = 1024\n",
    "            hop_s = 128\n",
    "            n_filters = 40\n",
    "            n_coeffs = 65\n",
    "            mfccs = zeros([n_coeffs,])\n",
    "            p = aubio.pvoc(win_s, hop_s)\n",
    "            m = aubio.mfcc(win_s, n_filters, n_coeffs, sr)\n",
    "            audio_idx = 0\n",
    "            audio = audio.astype('float32')\n",
    "            print(max(audio))\n",
    "            while True:\n",
    "                spec = p(audio[audio_idx: audio_idx + hop_s])\n",
    "                mfcc_out = m(spec)\n",
    "                mfcc_out = mfcc_out / np.max(np.abs(mfcc_out), axis=0)\n",
    "                mfccs = vstack((mfccs, mfcc_out))\n",
    "                audio_idx += hop_s\n",
    "                if audio_idx > len(audio) - hop_s: break\n",
    "            mfccs /= np.max(np.abs(mfccs))\n",
    "            features.append(mfccs)\n",
    "            # aubio.mfcc(buf_size=1024, n_filters=40, n_coeffs=13, samplerate=44100)\n",
    "        elif image_gen_method == \"stft\":\n",
    "            # stft = librosa.stft(y=audio, n_fft=1024, hop_length=128)\n",
    "            stft = mySTFT(y=audio, n_fft=1024, hop_length=128)\n",
    "            stft = np.abs(stft)\n",
    "            stft /= np.max(np.abs(stft))\n",
    "            features.append(stft)\n",
    "            if STFT_noise == True:\n",
    "                features.append(randomSTFT(stft))\n",
    "                features.append(randomSTFT(stft))\n",
    "        elif image_gen_method == \"stft_old\":\n",
    "            stft = librosa.stft(y=audio, n_fft=128, hop_length=32)\n",
    "            # stft = mySTFT(y=audio, n_fft=128, hop_length=32)\n",
    "            stft = np.abs(stft)\n",
    "            stft /= np.max(np.abs(stft))\n",
    "            features.append(stft)\n",
    "            if STFT_noise == True:\n",
    "                features.append(randomSTFT(stft))\n",
    "                features.append(randomSTFT(stft))\n",
    "        elif image_gen_method == \"raw\":\n",
    "            features.append(audio)\n",
    "            # Input(shape=(X_train.shape[1])),\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01648582 0.01900818 0.05903508 ... 0.00185016 0.00211154 0.00178479]\n",
      " [0.01762229 0.00432757 0.087982   ... 0.00916009 0.01054156 0.01062439]\n",
      " [0.00106062 0.02164529 0.11845702 ... 0.01957028 0.02212759 0.02271729]\n",
      " ...\n",
      " [0.00369735 0.00823158 0.0532215  ... 0.00243122 0.00087248 0.00153017]\n",
      " [0.00343356 0.00597678 0.05287816 ... 0.00225437 0.00110939 0.00015992]\n",
      " [0.00694383 0.00926696 0.04211648 ... 0.00194667 0.00141773 0.00058374]]\n"
     ]
    }
   ],
   "source": [
    "# mfcc result printer\n",
    "\n",
    "# test_audio, sr = librosa.load(\"./PA_sound_normalize/20240511_phone/drum/drum0_0.wav\") \n",
    "# print(max(test_audio))\n",
    "# # mfcc = librosa.feature.mfcc(y=test_audio, sr=sr, n_mfcc=60, hop_length=64)\n",
    "# mfcc = librosa.feature.mfcc(y=test_audio, sr=sr, n_mels=128)\n",
    "\n",
    "# print(mfcc)\n",
    "# mfcc = mfcc / np.max(np.abs(mfcc), axis=0)\n",
    "\n",
    "# stft result\n",
    "test_audio, sr = librosa.load(\"./PA_sound_normalize/20240511_phone/drum/drum0_0.wav\")\n",
    "# print(max(test_audio))\n",
    "# print(test_audio[0:8])\n",
    "# print(np.fft.rfft(test_audio[0:1024]))\n",
    "# origin_stft = librosa.stft(y=test_audio, n_fft=1024, hop_length=128)\n",
    "origin_stft = mySTFT(y=test_audio, n_fft=1024, hop_length=128)\n",
    "origin_stft /= np.max(origin_stft)\n",
    "print(origin_stft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9K325TWeG6W"
   },
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSmRFu2cdpzd"
   },
   "source": [
    "- features : call dataLoader\n",
    "- labels:\n",
    "    - 0 : meat\n",
    "    - 1 : drum\n",
    "    - 2 : column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83999,
     "status": "ok",
     "timestamp": 1714021170995,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "LWSNVu3DUL-7",
    "outputId": "9d12a90a-43d0-4d77-c761-791136bd713d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Files : 785\n",
      "Length of training features: 3294\n",
      "Length of validation features: 474\n",
      "Length of test features: 157\n",
      "#Files : 630\n",
      "Length of training features: 5940\n",
      "Length of validation features: 852\n",
      "Length of test features: 283\n",
      "#Files : 417\n",
      "Length of training features: 7686\n",
      "Length of validation features: 1104\n",
      "Length of test features: 367\n"
     ]
    }
   ],
   "source": [
    "# ceramics_paths = [\"./data/cutData/ceramics/\"]\n",
    "# drum_paths = [\"../cup/data/cutData/metal/\"]\n",
    "# col_paths = [\"../cup/data/cutData/plastic/\"]\n",
    "# meat_paths = [\"../cup/data/cutData/glass/\"]\n",
    "\n",
    "# drum_paths = [\"./PA_sound_normalize/20240511_phone/drum/\"]\n",
    "# col_paths = [\"./PA_sound_normalize/20240511_phone/column/\"]\n",
    "# meat_paths = [\"./PA_sound_normalize/20240511_phone/meat/\"]\n",
    "\n",
    "drum_paths = [\"../../bowen/phone_model/data/20240511_phone/drum/\"]\n",
    "col_paths = [\"../../bowen/phone_model/data/20240511_phone/column/\"]\n",
    "meat_paths = [\"../../bowen/phone_model/data/20240511_phone/meat/\"]\n",
    "\n",
    "audio_aug = True\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "val_features = []\n",
    "val_labels = []\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "for drum_path in drum_paths:\n",
    "    files = os.listdir(drum_path)\n",
    "    print(\"#Files : \" + str(len(files)))\n",
    "\n",
    "    # 打乱文件列表\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # 分割文件列表，70%用于训练，10%用于验证，20%用于测试\n",
    "    total_files = len(files)\n",
    "    train_split_index = int(0.7 * total_files)\n",
    "    val_split_index = int(0.8 * total_files)\n",
    "\n",
    "    train_files = files[:train_split_index]\n",
    "    val_files = files[train_split_index:val_split_index]\n",
    "    test_files = files[val_split_index:]\n",
    "\n",
    "    # 加载训练数据\n",
    "    for file in train_files:\n",
    "        try:\n",
    "            data_list = dataloader(drum_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in data_list:\n",
    "                train_features.append(data)\n",
    "                train_labels.append(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing training file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载验证数据\n",
    "    for file in val_files:\n",
    "        try:\n",
    "            val_data_list = dataloader(drum_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in val_data_list:\n",
    "                val_features.append(data)\n",
    "                val_labels.append(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing validation file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载测试数据\n",
    "    for file in test_files:\n",
    "        try:\n",
    "            test_data_list = dataloader(drum_path, file, shift=False, noise=False)\n",
    "            for data in test_data_list:\n",
    "                test_features.append(data)\n",
    "                test_labels.append(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Length of training features:\", len(train_features))\n",
    "    print(\"Length of validation features:\", len(val_features))\n",
    "    print(\"Length of test features:\", len(test_features))\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "for col_path in col_paths:\n",
    "    files = os.listdir(col_path)\n",
    "    print(\"#Files : \" + str(len(files)))\n",
    "\n",
    "    # 打乱文件列表\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # 分割文件列表，70%用于训练，10%用于验证，20%用于测试\n",
    "    total_files = len(files)\n",
    "    train_split_index = int(0.7 * total_files)\n",
    "    val_split_index = int(0.8 * total_files)\n",
    "\n",
    "    train_files = files[:train_split_index]\n",
    "    val_files = files[train_split_index:val_split_index]\n",
    "    test_files = files[val_split_index:]\n",
    "\n",
    "    # 加载训练数据\n",
    "    for file in train_files:\n",
    "        try:\n",
    "            data_list = dataloader(col_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in data_list:\n",
    "                train_features.append(data)\n",
    "                train_labels.append(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing training file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载验证数据\n",
    "    for file in val_files:\n",
    "        try:\n",
    "            val_data_list = dataloader(col_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in val_data_list:\n",
    "                val_features.append(data)\n",
    "                val_labels.append(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing validation file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载测试数据\n",
    "    for file in test_files:\n",
    "        try:\n",
    "            test_data_list = dataloader(col_path, file, shift=False, noise=False)\n",
    "            for data in test_data_list:\n",
    "                test_features.append(data)\n",
    "                test_labels.append(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Length of training features:\", len(train_features))\n",
    "    print(\"Length of validation features:\", len(val_features))\n",
    "    print(\"Length of test features:\", len(test_features))\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "for meat_path in meat_paths:\n",
    "    files = os.listdir(meat_path)\n",
    "    print(\"#Files : \" + str(len(files)))\n",
    "\n",
    "    # 打乱文件列表\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # 分割文件列表，70%用于训练，10%用于验证，20%用于测试\n",
    "    total_files = len(files)\n",
    "    train_split_index = int(0.7 * total_files)\n",
    "    val_split_index = int(0.8 * total_files)\n",
    "\n",
    "    train_files = files[:train_split_index]\n",
    "    val_files = files[train_split_index:val_split_index]\n",
    "    test_files = files[val_split_index:]\n",
    "\n",
    "    # 加载训练数据\n",
    "    for file in train_files:\n",
    "        try:\n",
    "            data_list = dataloader(meat_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in data_list:\n",
    "                train_features.append(data)\n",
    "                train_labels.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing training file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载验证数据\n",
    "    for file in val_files:\n",
    "        try:\n",
    "            val_data_list = dataloader(meat_path, file, shift=audio_aug, noise=audio_aug, STFT_noise=True)\n",
    "            for data in val_data_list:\n",
    "                val_features.append(data)\n",
    "                val_labels.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing validation file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 加载测试数据\n",
    "    for file in test_files:\n",
    "        try:\n",
    "            test_data_list = dataloader(meat_path, file, shift=False, noise=False)\n",
    "            for data in test_data_list:\n",
    "                test_features.append(data)\n",
    "                test_labels.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Length of training features:\", len(train_features))\n",
    "    print(\"Length of validation features:\", len(val_features))\n",
    "    print(\"Length of test features:\", len(test_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gaiik2HTaFJS"
   },
   "outputs": [],
   "source": [
    "# Some elements maybe wrong?\n",
    "filtered_image_list = []\n",
    "filtered_label_list = []\n",
    "\n",
    "\n",
    "for image, label in zip(train_features, train_labels):\n",
    "    if image.shape == (18, 513):\n",
    "        filtered_image_list.append(image)\n",
    "        filtered_label_list.append(label)\n",
    "    else:\n",
    "        print(f\"{image.shape} be skipped.\")\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "X_train = np.array(filtered_image_list)\n",
    "y_train = np.array(filtered_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements maybe wrong?\n",
    "filtered_image_list = []\n",
    "filtered_label_list = []\n",
    "\n",
    "for image, label in zip(val_features, val_labels):\n",
    "    if image.shape == (18,513):\n",
    "        filtered_image_list.append(image)\n",
    "        filtered_label_list.append(label)\n",
    "    else:\n",
    "        print(f\"{image.shape} be skipped.\")\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "X_val = np.array(filtered_image_list)\n",
    "y_val = np.array(filtered_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements maybe wrong?\n",
    "filtered_image_list = []\n",
    "filtered_label_list = []\n",
    "\n",
    "for image, label in zip(test_features, test_labels):\n",
    "    if image.shape == (18,513):\n",
    "        filtered_image_list.append(image)\n",
    "        filtered_label_list.append(label)\n",
    "    else:\n",
    "        print(f\"{image.shape} be skipped.\")\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "X_test = np.array(filtered_image_list)\n",
    "y_test = np.array(filtered_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 18, 513)\n",
      "(15120,)\n",
      "(2178, 18, 513)\n",
      "(2178,)\n",
      "(241, 18, 513)\n",
      "(241,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode => one-hot\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "\n",
    "y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "y_val_onehot = to_categorical(y_val_encoded)\n",
    "\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "y_test_onehot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4aqtaPOfp3O"
   },
   "source": [
    "## WanDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "error",
     "timestamp": 1714021252250,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "OjkSW3beU38s",
    "outputId": "553d16da-ae94-465c-9e0e-8e1bc71cffe0"
   },
   "outputs": [],
   "source": [
    "DATASET = \"manual\"    # \"auto or \"manual\"\n",
    "DATE = \"20240926\"\n",
    "MODEL = \"NN\"\n",
    "VERSION = \"0.0.1\"\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIj1qov-eQ_Z"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "181/181 [==============================] - 2s 10ms/step - loss: 0.9061 - accuracy: 0.5427 - val_loss: 0.8223 - val_accuracy: 0.5830\n",
      "Epoch 2/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.7450 - accuracy: 0.6585 - val_loss: 0.7232 - val_accuracy: 0.6673\n",
      "Epoch 3/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.6713 - accuracy: 0.7008 - val_loss: 0.6781 - val_accuracy: 0.6866\n",
      "Epoch 4/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.6270 - accuracy: 0.7180 - val_loss: 0.6662 - val_accuracy: 0.6932\n",
      "Epoch 5/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.5927 - accuracy: 0.7399 - val_loss: 0.6629 - val_accuracy: 0.7098\n",
      "Epoch 6/50\n",
      "181/181 [==============================] - 2s 9ms/step - loss: 0.5630 - accuracy: 0.7612 - val_loss: 0.6353 - val_accuracy: 0.7147\n",
      "Epoch 7/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.5355 - accuracy: 0.7737 - val_loss: 0.6064 - val_accuracy: 0.7331\n",
      "Epoch 8/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.5009 - accuracy: 0.7948 - val_loss: 0.6017 - val_accuracy: 0.7240\n",
      "Epoch 9/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.4770 - accuracy: 0.8045 - val_loss: 0.6033 - val_accuracy: 0.7319\n",
      "Epoch 10/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.4530 - accuracy: 0.8144 - val_loss: 0.5979 - val_accuracy: 0.7409\n",
      "Epoch 11/50\n",
      "181/181 [==============================] - 2s 9ms/step - loss: 0.4333 - accuracy: 0.8246 - val_loss: 0.5829 - val_accuracy: 0.7500\n",
      "Epoch 12/50\n",
      "181/181 [==============================] - 2s 9ms/step - loss: 0.4111 - accuracy: 0.8337 - val_loss: 0.5753 - val_accuracy: 0.7591\n",
      "Epoch 13/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.3904 - accuracy: 0.8410 - val_loss: 0.5712 - val_accuracy: 0.7548\n",
      "Epoch 14/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.3706 - accuracy: 0.8522 - val_loss: 0.5728 - val_accuracy: 0.7542\n",
      "Epoch 15/50\n",
      "181/181 [==============================] - 2s 9ms/step - loss: 0.3557 - accuracy: 0.8578 - val_loss: 0.5985 - val_accuracy: 0.7512\n",
      "Epoch 16/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.3381 - accuracy: 0.8653 - val_loss: 0.5897 - val_accuracy: 0.7560\n",
      "Epoch 17/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.3203 - accuracy: 0.8726 - val_loss: 0.5713 - val_accuracy: 0.7696\n",
      "Epoch 18/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.3081 - accuracy: 0.8798 - val_loss: 0.5892 - val_accuracy: 0.7663\n",
      "Epoch 19/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.2948 - accuracy: 0.8843 - val_loss: 0.5867 - val_accuracy: 0.7633\n",
      "Epoch 20/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.2885 - accuracy: 0.8860 - val_loss: 0.5751 - val_accuracy: 0.7720\n",
      "Epoch 21/50\n",
      "181/181 [==============================] - 2s 9ms/step - loss: 0.2747 - accuracy: 0.8909 - val_loss: 0.6038 - val_accuracy: 0.7705\n",
      "Epoch 22/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.2606 - accuracy: 0.8973 - val_loss: 0.5947 - val_accuracy: 0.7696\n",
      "Epoch 23/50\n",
      "181/181 [==============================] - 2s 8ms/step - loss: 0.2475 - accuracy: 0.9003 - val_loss: 0.5775 - val_accuracy: 0.7853\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.4700 - accuracy: 0.8011\n",
      "測試損失: 0.4699949324131012, 測試準確度: 0.8010899424552917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 調整輸入形狀以適應 CNN (添加通道維度)\n",
    "X_train = X_train.reshape(-1, 513, 18, 1)\n",
    "X_val = X_val.reshape(-1, 513, 18, 1)\n",
    "X_test = X_test.reshape(-1, 513, 18, 1)\n",
    "\n",
    "# 構建 CNN 模型\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(513, 18, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 使用 Adam 優化器\n",
    "adam = Adam(lr=0.0002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# 定義回調函數\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "checkpoint_path = f\"./checkpoint/model_checkpoint.ckpt\"\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(X_train, y_train_onehot, epochs=50, batch_size=128, validation_data=(X_val, y_val_onehot), callbacks=callbacks)\n",
    "\n",
    "# 在測試集上評估模型\n",
    "loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgpbbrEwsDzE"
   },
   "source": [
    "### Ver 1.0.0 RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145231,
     "status": "ok",
     "timestamp": 1713449796296,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "eKmkuVyRVHDq",
    "outputId": "744d7794-336b-4f51-abbc-62e50cf1a52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 1.0620 - accuracy: 0.4562 - val_loss: 1.0371 - val_accuracy: 0.4737\n",
      "Epoch 2/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 1.0318 - accuracy: 0.4940 - val_loss: 1.0314 - val_accuracy: 0.4828\n",
      "Epoch 3/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 1.0126 - accuracy: 0.5052 - val_loss: 1.0320 - val_accuracy: 0.4665\n",
      "Epoch 4/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 1.0023 - accuracy: 0.5070 - val_loss: 1.0146 - val_accuracy: 0.4819\n",
      "Epoch 5/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.9875 - accuracy: 0.5167 - val_loss: 1.0133 - val_accuracy: 0.5118\n",
      "Epoch 6/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.9726 - accuracy: 0.5323 - val_loss: 1.0209 - val_accuracy: 0.4801\n",
      "Epoch 7/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.9620 - accuracy: 0.5385 - val_loss: 0.9823 - val_accuracy: 0.5217\n",
      "Epoch 8/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.9448 - accuracy: 0.5517 - val_loss: 0.9702 - val_accuracy: 0.5290\n",
      "Epoch 9/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.9290 - accuracy: 0.5600 - val_loss: 0.9009 - val_accuracy: 0.5842\n",
      "Epoch 10/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.8834 - accuracy: 0.5974 - val_loss: 0.7965 - val_accuracy: 0.6476\n",
      "Epoch 11/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.8136 - accuracy: 0.6340 - val_loss: 0.7458 - val_accuracy: 0.6513\n",
      "Epoch 12/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.7515 - accuracy: 0.6593 - val_loss: 0.6996 - val_accuracy: 0.6803\n",
      "Epoch 13/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.7375 - accuracy: 0.6674 - val_loss: 0.7002 - val_accuracy: 0.6712\n",
      "Epoch 14/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.7097 - accuracy: 0.6828 - val_loss: 0.7357 - val_accuracy: 0.6703\n",
      "Epoch 15/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.6900 - accuracy: 0.6929 - val_loss: 0.6948 - val_accuracy: 0.6594\n",
      "Epoch 16/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.6757 - accuracy: 0.7019 - val_loss: 0.7784 - val_accuracy: 0.6467\n",
      "Epoch 17/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.6556 - accuracy: 0.7090 - val_loss: 0.6918 - val_accuracy: 0.6585\n",
      "Epoch 18/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.6365 - accuracy: 0.7234 - val_loss: 0.7418 - val_accuracy: 0.6621\n",
      "Epoch 19/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.6390 - accuracy: 0.7208 - val_loss: 0.6881 - val_accuracy: 0.6676\n",
      "Epoch 20/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.6067 - accuracy: 0.7309 - val_loss: 0.6848 - val_accuracy: 0.6748\n",
      "Epoch 21/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5974 - accuracy: 0.7406 - val_loss: 0.6672 - val_accuracy: 0.6929\n",
      "Epoch 22/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5801 - accuracy: 0.7440 - val_loss: 0.6973 - val_accuracy: 0.6920\n",
      "Epoch 23/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5684 - accuracy: 0.7475 - val_loss: 0.6987 - val_accuracy: 0.6803\n",
      "Epoch 24/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5634 - accuracy: 0.7590 - val_loss: 0.6684 - val_accuracy: 0.6993\n",
      "Epoch 25/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5611 - accuracy: 0.7645 - val_loss: 0.6808 - val_accuracy: 0.6920\n",
      "Epoch 26/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5478 - accuracy: 0.7601 - val_loss: 0.6475 - val_accuracy: 0.7029\n",
      "Epoch 27/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5338 - accuracy: 0.7723 - val_loss: 0.6732 - val_accuracy: 0.7092\n",
      "Epoch 28/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5233 - accuracy: 0.7745 - val_loss: 0.6634 - val_accuracy: 0.6993\n",
      "Epoch 29/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5102 - accuracy: 0.7753 - val_loss: 0.6665 - val_accuracy: 0.7138\n",
      "Epoch 30/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.5025 - accuracy: 0.7843 - val_loss: 0.6632 - val_accuracy: 0.7138\n",
      "Epoch 31/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4904 - accuracy: 0.7918 - val_loss: 0.6919 - val_accuracy: 0.7065\n",
      "Epoch 32/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.5021 - accuracy: 0.7855 - val_loss: 0.6700 - val_accuracy: 0.7029\n",
      "Epoch 33/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4865 - accuracy: 0.7899 - val_loss: 0.7120 - val_accuracy: 0.7138\n",
      "Epoch 34/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4806 - accuracy: 0.8015 - val_loss: 0.6590 - val_accuracy: 0.7056\n",
      "Epoch 35/50\n",
      "241/241 [==============================] - 3s 13ms/step - loss: 0.4771 - accuracy: 0.7940 - val_loss: 0.6765 - val_accuracy: 0.7138\n",
      "Epoch 36/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4605 - accuracy: 0.8087 - val_loss: 0.6799 - val_accuracy: 0.7056\n",
      "Epoch 37/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4564 - accuracy: 0.8078 - val_loss: 0.6920 - val_accuracy: 0.6984\n",
      "Epoch 38/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4635 - accuracy: 0.8054 - val_loss: 0.6735 - val_accuracy: 0.6957\n",
      "Epoch 39/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4547 - accuracy: 0.8082 - val_loss: 0.6887 - val_accuracy: 0.7074\n",
      "Epoch 40/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4462 - accuracy: 0.8089 - val_loss: 0.7420 - val_accuracy: 0.7264\n",
      "Epoch 41/50\n",
      "241/241 [==============================] - 3s 12ms/step - loss: 0.4440 - accuracy: 0.8078 - val_loss: 0.6646 - val_accuracy: 0.7074\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.6646 - accuracy: 0.7074\n",
      "測試損失: 0.6645910143852234, 測試準確度: 0.7074275612831116\n"
     ]
    }
   ],
   "source": [
    "# X : features ，y : label（'Drum sound' or 'Meat sound'）\n",
    "\n",
    "# encode => one-hot\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# # split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# define model\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0005, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train_onehot, epochs=50, batch_size=32, validation_data=(X_val, y_val_onehot),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# evaluate\n",
    "loss, accuracy = model.evaluate(X_val, y_val_onehot)\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")\n",
    "\n",
    "# prediction\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7ybWf6httvd"
   },
   "source": [
    "### Ver 2.0.0 LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111282,
     "status": "ok",
     "timestamp": 1714024749766,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "FkUubd-Jtxln",
    "outputId": "93954814-34c4-466c-8828-e1f01d55ce8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/300\n",
      "241/241 [==============================] - 6s 23ms/step - loss: 1.0504 - accuracy: 0.4452 - val_loss: 1.0149 - val_accuracy: 0.5308\n",
      "Epoch 2/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.8975 - accuracy: 0.5805 - val_loss: 0.8497 - val_accuracy: 0.6123\n",
      "Epoch 3/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.7750 - accuracy: 0.6586 - val_loss: 0.7513 - val_accuracy: 0.6694\n",
      "Epoch 4/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.7205 - accuracy: 0.6868 - val_loss: 0.6955 - val_accuracy: 0.7020\n",
      "Epoch 5/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.6829 - accuracy: 0.7015 - val_loss: 0.7326 - val_accuracy: 0.6821\n",
      "Epoch 6/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.6644 - accuracy: 0.7087 - val_loss: 0.6891 - val_accuracy: 0.7083\n",
      "Epoch 7/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.6507 - accuracy: 0.7187 - val_loss: 0.6339 - val_accuracy: 0.7219\n",
      "Epoch 8/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.6220 - accuracy: 0.7283 - val_loss: 0.6462 - val_accuracy: 0.7255\n",
      "Epoch 9/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.6165 - accuracy: 0.7311 - val_loss: 0.6434 - val_accuracy: 0.7156\n",
      "Epoch 10/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.6069 - accuracy: 0.7426 - val_loss: 0.6393 - val_accuracy: 0.7074\n",
      "Epoch 11/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5825 - accuracy: 0.7514 - val_loss: 0.6230 - val_accuracy: 0.7083\n",
      "Epoch 12/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.5736 - accuracy: 0.7532 - val_loss: 0.6327 - val_accuracy: 0.7029\n",
      "Epoch 13/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5551 - accuracy: 0.7652 - val_loss: 0.6155 - val_accuracy: 0.7120\n",
      "Epoch 14/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.5492 - accuracy: 0.7662 - val_loss: 0.6268 - val_accuracy: 0.7255\n",
      "Epoch 15/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.5412 - accuracy: 0.7679 - val_loss: 0.6357 - val_accuracy: 0.7274\n",
      "Epoch 16/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5312 - accuracy: 0.7757 - val_loss: 0.6220 - val_accuracy: 0.7400\n",
      "Epoch 17/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5260 - accuracy: 0.7784 - val_loss: 0.6022 - val_accuracy: 0.7274\n",
      "Epoch 18/300\n",
      "241/241 [==============================] - 6s 23ms/step - loss: 0.5248 - accuracy: 0.7762 - val_loss: 0.5978 - val_accuracy: 0.7292\n",
      "Epoch 19/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5125 - accuracy: 0.7796 - val_loss: 0.6009 - val_accuracy: 0.7237\n",
      "Epoch 20/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.5036 - accuracy: 0.7857 - val_loss: 0.5825 - val_accuracy: 0.7400\n",
      "Epoch 21/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4941 - accuracy: 0.7947 - val_loss: 0.5999 - val_accuracy: 0.7319\n",
      "Epoch 22/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4934 - accuracy: 0.7918 - val_loss: 0.5960 - val_accuracy: 0.7437\n",
      "Epoch 23/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4887 - accuracy: 0.7910 - val_loss: 0.6310 - val_accuracy: 0.7174\n",
      "Epoch 24/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4776 - accuracy: 0.7983 - val_loss: 0.6024 - val_accuracy: 0.7337\n",
      "Epoch 25/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4677 - accuracy: 0.8008 - val_loss: 0.5896 - val_accuracy: 0.7418\n",
      "Epoch 26/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4712 - accuracy: 0.8051 - val_loss: 0.5841 - val_accuracy: 0.7409\n",
      "Epoch 27/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4641 - accuracy: 0.7973 - val_loss: 0.5819 - val_accuracy: 0.7473\n",
      "Epoch 28/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4570 - accuracy: 0.8046 - val_loss: 0.5648 - val_accuracy: 0.7518\n",
      "Epoch 29/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4555 - accuracy: 0.8054 - val_loss: 0.5940 - val_accuracy: 0.7346\n",
      "Epoch 30/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4481 - accuracy: 0.8091 - val_loss: 0.6419 - val_accuracy: 0.7264\n",
      "Epoch 31/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4416 - accuracy: 0.8090 - val_loss: 0.5871 - val_accuracy: 0.7292\n",
      "Epoch 32/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4471 - accuracy: 0.8124 - val_loss: 0.5977 - val_accuracy: 0.7382\n",
      "Epoch 33/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4365 - accuracy: 0.8134 - val_loss: 0.5845 - val_accuracy: 0.7382\n",
      "Epoch 34/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4272 - accuracy: 0.8143 - val_loss: 0.5565 - val_accuracy: 0.7572\n",
      "Epoch 35/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4301 - accuracy: 0.8152 - val_loss: 0.5568 - val_accuracy: 0.7600\n",
      "Epoch 36/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4216 - accuracy: 0.8177 - val_loss: 0.5762 - val_accuracy: 0.7500\n",
      "Epoch 37/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4162 - accuracy: 0.8199 - val_loss: 0.5642 - val_accuracy: 0.7373\n",
      "Epoch 38/300\n",
      "241/241 [==============================] - 6s 23ms/step - loss: 0.4137 - accuracy: 0.8264 - val_loss: 0.5567 - val_accuracy: 0.7545\n",
      "Epoch 39/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.4094 - accuracy: 0.8253 - val_loss: 0.5681 - val_accuracy: 0.7446\n",
      "Epoch 40/300\n",
      "241/241 [==============================] - 6s 23ms/step - loss: 0.4042 - accuracy: 0.8283 - val_loss: 0.5595 - val_accuracy: 0.7500\n",
      "Epoch 41/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4014 - accuracy: 0.8281 - val_loss: 0.5893 - val_accuracy: 0.7446\n",
      "Epoch 42/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.4004 - accuracy: 0.8285 - val_loss: 0.5727 - val_accuracy: 0.7545\n",
      "Epoch 43/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.3928 - accuracy: 0.8305 - val_loss: 0.5928 - val_accuracy: 0.7382\n",
      "Epoch 44/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.3918 - accuracy: 0.8316 - val_loss: 0.5685 - val_accuracy: 0.7428\n",
      "Epoch 45/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.3890 - accuracy: 0.8314 - val_loss: 0.5790 - val_accuracy: 0.7418\n",
      "Epoch 46/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.3869 - accuracy: 0.8316 - val_loss: 0.5758 - val_accuracy: 0.7545\n",
      "Epoch 47/300\n",
      "241/241 [==============================] - 5s 22ms/step - loss: 0.3861 - accuracy: 0.8365 - val_loss: 0.5795 - val_accuracy: 0.7636\n",
      "Epoch 48/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.3789 - accuracy: 0.8384 - val_loss: 0.6024 - val_accuracy: 0.7464\n",
      "Epoch 49/300\n",
      "241/241 [==============================] - 5s 23ms/step - loss: 0.3741 - accuracy: 0.8391 - val_loss: 0.5814 - val_accuracy: 0.7509\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5814 - accuracy: 0.7509\n",
      "測試損失: 0.5813952088356018, 測試準確度: 0.7509058117866516\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# X : features ，y : label（'Drum sound' or 'Meat sound'）\n",
    "\n",
    "# # encode => one-hot\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "# print(y_train[0])\n",
    "# define model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu', recurrent_dropout=0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0005, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# TODO:\n",
    "# Create callbacks:\n",
    "# 1. early stopping (with a patience of 5 epochs)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "# 2. model checkpointing\n",
    "#checkpoint_path = f\"../checkpoint/{MODEL}/{DATE}_{MODEL}_v{VERSION}.ckpt\"\n",
    "#model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True)\n",
    "# 3. learning rate reduction on plateau\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n",
    "#           callbacks=[WandbCallback(), early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train_onehot, epochs=300, batch_size=32, validation_data=(X_val, y_val_onehot),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# evaluate\n",
    "loss, accuracy = model.evaluate(X_val, y_val_onehot)\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.3589 - accuracy: 0.8574 - val_loss: 0.3505 - val_accuracy: 0.8792\n",
      "Epoch 2/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9212 - val_loss: 0.3630 - val_accuracy: 0.8669\n",
      "Epoch 3/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9376 - val_loss: 0.3526 - val_accuracy: 0.8838\n",
      "Epoch 4/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9468 - val_loss: 0.3569 - val_accuracy: 0.8733\n",
      "Epoch 5/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9524 - val_loss: 0.3989 - val_accuracy: 0.8710\n",
      "Epoch 6/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.1065 - accuracy: 0.9597 - val_loss: 0.3596 - val_accuracy: 0.8898\n",
      "Epoch 7/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9653 - val_loss: 0.4385 - val_accuracy: 0.8976\n",
      "Epoch 8/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9673 - val_loss: 0.3581 - val_accuracy: 0.8884\n",
      "Epoch 9/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.9688 - val_loss: 0.8302 - val_accuracy: 0.7599\n",
      "Epoch 10/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9711 - val_loss: 0.4566 - val_accuracy: 0.8843\n",
      "Epoch 11/500\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9767 - val_loss: 0.4217 - val_accuracy: 0.8875\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.4217 - accuracy: 0.8875\n",
      "測試損失: 0.4216867685317993, 測試準確度: 0.8875114917755127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# X : features ，y : label（'Drum sound' or 'Meat sound'）\n",
    "\n",
    "#X_train1, X_val, y_train_onehot1, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# define model\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#     Dense(512, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'),\n",
    "#     Dropout(0.1),\n",
    "#     Dense(y_onehot.shape[1], activation='softmax')\n",
    "# ])\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2]), name='input_2'),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# TODO:\n",
    "# Create callbacks:\n",
    "# 1. early stopping (with a patience of 5 epochs)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# 2. model checkpointing\n",
    "#checkpoint_path = f\"../checkpoint/{MODEL}/{DATE}_{MODEL}_v{VERSION}.ckpt\"\n",
    "#model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True)\n",
    "# 3. learning rate reduction on plateau\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val),\n",
    "#           callbacks=[WandbCallback(), early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train_onehot, epochs=500, batch_size=batch_size, validation_data=(X_val, y_val_onehot),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# evaluate\n",
    "loss, accuracy = model.evaluate(X_val, y_val_onehot)\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9627\n",
      "測試損失: 0.10038008540868759, 測試準確度: 0.9626556038856506\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Activation, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.regularizers import l2\n",
    "\n",
    "# # X : features ，y : label（'Drum sound' or 'Meat sound'）\n",
    "\n",
    "# # encode => one-hot\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=40)\n",
    "# X_train = X_train[..., np.newaxis]\n",
    "# y_train = y_train[..., np.newaxis]\n",
    "# print(y_train.shape)\n",
    "# # define model\n",
    "# model = Sequential([\n",
    "#     Conv2D(64, kernel_size=(2, 2), activation='relu', input_shape=(5, 60, 1)),\n",
    "#     Conv2D(192, (2, 2)),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(32, (2, 2)),\n",
    "#     Activation('relu'),\n",
    "#     # Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.25\n",
    "#     Dropout(0.25),\n",
    "#     # Flatten層把多維的輸入一維化，常用在從卷積層到全連接層的過渡。\n",
    "#     Flatten(),\n",
    "#     # 全連接層: 128個output\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.25),\n",
    "#     # Add output layer\n",
    "#     Dense(2, activation='softmax')\n",
    "# ])\n",
    "# adam = Adam(lr=0.02, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# # TODO:\n",
    "# # Create callbacks:\n",
    "# # 1. early stopping (with a patience of 5 epochs)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "# # 2. model checkpointing\n",
    "# checkpoint_path = f\"../checkpoint/{MODEL}/{DATE}_{MODEL}_v{VERSION}.ckpt\"\n",
    "# model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True)\n",
    "# # 3. learning rate reduction on plateau\n",
    "# # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "\n",
    "# # model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n",
    "# #           callbacks=[WandbCallback(), early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "\n",
    "# # train model\n",
    "# model.fit(X_train, y_train, epochs=300, batch_size=128, validation_data=(X_test, y_test),\n",
    "#           callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# # evaluate\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYCygX-XkhYl"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1KL9lXxrkow1"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1168,
     "status": "ok",
     "timestamp": 1714024843315,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "NRT2R9PCjta1",
    "outputId": "b63e00d3-35e7-4ea5-d9f4-ed9604e6d86d"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_1 = []\n",
    "y_test_1 = []\n",
    "for i in y_pred:\n",
    "    max_id = 0\n",
    "    max_val = 0\n",
    "    for j in range(2):\n",
    "        if i[j] > 0.5:\n",
    "            max_id = j\n",
    "            max_val = i[j]\n",
    "    y_pred_1.append(max_id)\n",
    "\n",
    "for i in y_test_onehot:\n",
    "    max_id = 0\n",
    "    max_val = 0\n",
    "    for j in range(2):\n",
    "        if max_val < i[j]:\n",
    "            max_id = j\n",
    "            max_val = i[j]\n",
    "    y_test_1.append(max_id)\n",
    "\n",
    "\n",
    "# print(y_test_1, y_pred, y_pred_1)\n",
    "# rounded_labels=np.argmax(y_test_1, axis=1)\n",
    "cf_matrix = confusion_matrix(y_test_1, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1714024845461,
     "user": {
      "displayName": "林奕安",
      "userId": "06087888647373566805"
     },
     "user_tz": -480
    },
    "id": "TLhXHbIVkjIS",
    "outputId": "32562154-3f95-4816-b668-67367cd15b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試損失: 0.46602320671081543, 測試準確度: 0.8228882551193237\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt=\"d\")\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['meat','drum', 'column'])\n",
    "ax.yaxis.set_ticklabels(['meat','drum', 'column'])\n",
    "\n",
    "print(f\"測試損失: {loss}, 測試準確度: {accuracy}\")\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt=\"d\")\n",
    "\n",
    "ax.set_title('Pineapple 2 types')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "# ax.xaxis.set_ticklabels(['Meat','Column','Drum'])\n",
    "# ax.yaxis.set_ticklabels(['Meat','Column','Drum'])\n",
    "# ax.xaxis.set_ticklabels(['Meat','Column','Drum', 'Unidentified'])\n",
    "# ax.yaxis.set_ticklabels(['Meat','Column','Drum', 'Unidentified'])\n",
    "ax.xaxis.set_ticklabels(['Meat','Drum'])\n",
    "ax.yaxis.set_ticklabels(['Meat','Drum'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model (h5)\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "filepath = f\"./model/\"\n",
    "if not os.path.isdir(filepath):\n",
    "    os.makedirs(filepath)\n",
    "save_model(model, filepath + f\"{DATE}_{MODEL}_v{VERSION}\", save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/pb/20240926_NN_v0.0.1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/pb/20240926_NN_v0.0.1/assets\n"
     ]
    }
   ],
   "source": [
    "# save model (pb)\n",
    "filepath = f\"./model/pb/{DATE}_{MODEL}_v{VERSION}\"\n",
    "if not os.path.isdir(filepath):\n",
    "    os.makedirs(filepath)\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 21:39:15.559614: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2024-09-26 21:39:15.559635: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2024-09-26 21:39:15.559639: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2024-09-26 21:39:15.559729: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: ./model/pb/20240926_NN_v0.0.1\n",
      "2024-09-26 21:39:15.561067: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2024-09-26 21:39:15.561076: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./model/pb/20240926_NN_v0.0.1\n",
      "2024-09-26 21:39:15.566099: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2024-09-26 21:39:15.605301: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: ./model/pb/20240926_NN_v0.0.1\n",
      "2024-09-26 21:39:15.611198: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 51470 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9525728"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert pb model to tflite model\n",
    "# saved_model_dir = '../../../model/20230325_auto/pb/20230325_VGG16_v2/'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(filepath)\n",
    "converter.experimental_new_converter = True\n",
    "tflite_model = converter.convert()\n",
    "open(f\"./model/{DATE}_{MODEL}_v{VERSION}.tflite\", 'wb').write(tflite_model)\n",
    "# open(filepath + \".tflite\", 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  18 513]\n",
      "[1 2]\n",
      "[[0.99843675 0.00156322]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=f\"./model/{DATE}_{MODEL}_v{VERSION}.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details[0]['shape'])\n",
    "print(output_details[0]['shape'])\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# 加载 ONNX 模型\n",
    "model = onnx.load(\"./pine1.onnx\")\n",
    "\n",
    "# 获取图的第一个输入节点\n",
    "model.graph.input[0].name = 'input_2:0'  # 将 input_1 改为 input_2\n",
    "\n",
    "# 保存修改后的 ONNX 模型\n",
    "onnx.save(model, \"./pine.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (2), usually from a call to set_ticks, does not match the number of ticklabels (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_379324/215369682.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m## Ticket labels - List must be in alphabetical order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Meat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Drum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Meat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Drum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/seedenv/lib/python3.9/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[0;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;31m# remove all tick labels, so only error for > 0 ticklabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1721\u001b[0m                     \u001b[0;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m                     \u001b[0;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of FixedLocator locations (2), usually from a call to set_ticks, does not match the number of ticklabels (3)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = []\n",
    "num = 0\n",
    "for data in X_test:\n",
    "    data = np.float32(data)\n",
    "    interpreter.set_tensor(input_details[0]['index'], [data])\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    num += 1\n",
    "    if output_data[0][0] >= 0.7:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "    #print(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test_1, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt=\"d\")\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Meat', 'Column', 'Drum'])\n",
    "ax.yaxis.set_ticklabels(['Meat', 'Column', 'Drum'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_model' from 'keras.utils' (/home/hsnl-iot/anaconda3/envs/seedenv/lib/python3.9/site-packages/keras/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1927888/443937777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_model' from 'keras.utils' (/home/hsnl-iot/anaconda3/envs/seedenv/lib/python3.9/site-packages/keras/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(100, 10), name='input_2'),  # Replace (100, 10) with your input shape\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')  # Replace 10 with the number of classes in your problem\n",
    "])\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrpBplfLB7Q+amquIbhAFr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "seedenv",
   "language": "python",
   "name": "seedenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
